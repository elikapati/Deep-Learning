{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regularization technique to avoid overfit and generalize model during training\n",
    "\n",
    "- Simple and powerful regularization technique to __avoid overfitting__ and __generalize model__ during training is to apply <font color=red>Dropout</font>\n",
    "- __Dropout__ is a technique where randomly selected neurons are ignored during training\n",
    "- Dropped out neurons do not participate in forward pass and backward pass\n",
    "- Dropout is applied only to training data\n",
    "- It can be applied to __Input__ layers and / or __Hidden__ layers\n",
    "- Applying dropout is believed to let model __represent patterns better__\n",
    "- Applying dropout makes model __less sensitive to weights__\n",
    "\n",
    "<font color=red>Tips</font>\n",
    "- Use a dropout of 20% to 50% of neurons\n",
    "- __Low dropout__ may have __low or no impact__\n",
    "- __High dropout__ may let model __under learn__\n",
    "- Suitable for complex, larger network\n",
    "- Apply dropout on __input__ layers as well as __hidden__ layers\n",
    "- Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n",
    "- Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline performance __before applying Dropout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 87.98% (3.86%)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('Data/sonar.all-data', header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "y = dataset[:, 60]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "\n",
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    sgd = SGD(lr=0.01, momentum=0.8)    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance after applying __Dropout on Input layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on applying dropout on input layer: 85.05% (8.04%)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60, )))\n",
    "    model.add(Dense(60, activation='relu', kernel_constraint=max_norm(3)))\n",
    "    model.add(Dense(30, activation='relu', kernel_constraint=max_norm(3)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    sgd = SGD(lr=0.1, momentum=0.9)    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "print(\"Performance on applying dropout on input layer: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice <font color=red>drop on performance</font> after applying dropout on <font color=red>input layer</font>:\n",
    "- Baseline:    88.50% (6.06%)\n",
    "- Input Layer: 86.50% (9.11%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance after applying __Dropout on Hidden layers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on applying dropout on hidden layers: 85.17% (6.11%)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, activation='relu', kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    sgd = SGD(lr=0.1, momentum=0.9)    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_y, cv=kfold)\n",
    "print(\"Performance on applying dropout on hidden layers: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice <font color=red>drop on performance</font> after applying dropout on input layer and <font color=red>hidden layers</font>:\n",
    "- Baseline:    88.50% (6.06%)\n",
    "- Input Layer: 86.50% (9.11%)\n",
    "- Hidden Layers: 85.05% (6.33%)\n",
    "\n",
    "Summary:\n",
    "- We can see that for this problem and for the chosen network configuration that using __dropout__ in the hidden layers __did not lift performance__. In fact, performance was worse than the baseline. \n",
    "- It is possible that additional training epochs are required or that further tuning is required to the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
